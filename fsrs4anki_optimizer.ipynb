{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FSRS4Anki v2.1.1 Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurCmW0Jqz3s"
      },
      "source": [
        "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/open-spaced-repetition/fsrs4anki/blob/v2.1.1/fsrs4anki_optimizer.ipynb)\n",
        "\n",
        "↑ Click the above button to open the optimizer on Google Colab.\n",
        "\n",
        "> If you can't see the button and are located in the Chinese Mainland, please use a proxy or VPN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG7bBfGJFbMr"
      },
      "source": [
        "Upload your **Anki Deck Package (.apkg)** file or **Anki Collection Package (.colpkg)** file on the `Left sidebar -> Files`, drag and drop your file in the current directory (not the `sample_data` directory). \n",
        "\n",
        "No need to include media. Need to include scheduling information. \n",
        "\n",
        "> If you use the latest version of Anki, please check the box `Support older Anki versions (slower/larger files)` when you export.\n",
        "\n",
        "You can export it via `File -> Export...` or `Ctrl + E` in the main window of Anki.\n",
        "\n",
        "Then replace the `filename` with yours in the next code cell. And set the `timezone` and `next_day_starts_at` which can be found in your preferences of Anki.\n",
        "\n",
        "After that, just run all (`Runtime -> Run all` or `Ctrl + F9`) and wait for minutes. You can see the optimal parameters in section **3 Result**. Copy them, replace the parameters in `fsrs4anki_scheduler.js`, and paste them into the custom scheduling of your deck options (require Anki version >= 2.1.55).\n",
        "\n",
        "**NOTE**: The default output is generated from my review logs. If you find the output is the same as mine, maybe your notebook hasn't run there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iqP70_-3EUhi"
      },
      "outputs": [],
      "source": [
        "# Here are some settings that you need to replace before running this optimizer.\n",
        "\n",
        "filename = \"ALL__Learning.apkg\"\n",
        "# If you upload deck file, replace it with your deck filename. E.g., ALL__Learning.apkg\n",
        "# If you upload collection file, replace it with your colpgk filename. E.g., collection-2022-09-18@13-21-58.colpkg\n",
        "\n",
        "# Replace it with your timezone. I'm in China, so I use Asia/Shanghai.\n",
        "timezone = 'Asia/Shanghai'\n",
        "\n",
        "# Replace it with your Anki's setting in Prefernces -> Scheduling.\n",
        "next_day_starts_at = 4\n",
        "\n",
        "# Replace it if you don't want the optimizer to use the review logs before a specific date.\n",
        "revlog_start_date = \"2006-10-05\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLFVNmG2qd06"
      },
      "source": [
        "## 1 Build dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkzFeKawqgbs"
      },
      "source": [
        "### 1.1 Extract Anki collection & deck file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD2js_wEr_Bs",
        "outputId": "42653d9e-316e-40bc-bd1d-f3a0e2b246c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extract successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "# Extract the collection file or deck file to get the .anki21 database.\n",
        "with zipfile.ZipFile(f'./{filename}', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n",
        "    print(\"Extract successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKpy4VfqGmaL"
      },
      "source": [
        "### 1.2 Create time-series feature\n",
        "\n",
        "The following code cell will extract the review logs from your Anki collection and preprocess them to a trainset which is saved in `revlog_history.tsv`.\n",
        "\n",
        " The time-series features are important in optimizing the model's parameters. For more detail, please see my paper: https://www.maimemo.com/paper/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2IIaY3PDaaG",
        "outputId": "607916c9-da95-48dd-fdab-6bd83fbbbb40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "revlog.csv saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5166/5166 [00:17<00:00, 299.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainset saved!\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import time\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import timedelta, datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "if os.path.isfile(\"collection.anki21b\"):\n",
        "    os.remove(\"collection.anki21b\")\n",
        "    raise Exception(\n",
        "        \"Please export the file with `support older Anki versions` if you use the latest version of Anki.\")\n",
        "elif os.path.isfile(\"collection.anki21\"):\n",
        "    con = sqlite3.connect(\"collection.anki21\")\n",
        "elif os.path.isfile(\"collection.anki2\"):\n",
        "    con = sqlite3.connect(\"collection.anki2\")\n",
        "else:\n",
        "    raise Exception(\"Collection not exist!\")\n",
        "cur = con.cursor()\n",
        "res = cur.execute(\"SELECT * FROM revlog\")\n",
        "revlog = res.fetchall()\n",
        "\n",
        "df = pd.DataFrame(revlog)\n",
        "df.columns = ['id', 'cid', 'usn', 'r', 'ivl',\n",
        "              'last_lvl', 'factor', 'time', 'type']\n",
        "df = df[(df['cid'] <= time.time() * 1000) &\n",
        "        (df['id'] <= time.time() * 1000) &\n",
        "        (df['id'] >= time.mktime(datetime.strptime(revlog_start_date, \"%Y-%m-%d\").timetuple()) * 1000)].copy()\n",
        "df['create_date'] = pd.to_datetime(df['cid'] // 1000, unit='s')\n",
        "df['create_date'] = df['create_date'].dt.tz_localize(\n",
        "    'UTC').dt.tz_convert(timezone)\n",
        "df['review_date'] = pd.to_datetime(df['id'] // 1000, unit='s')\n",
        "df['review_date'] = df['review_date'].dt.tz_localize(\n",
        "    'UTC').dt.tz_convert(timezone)\n",
        "df.drop(df[df['review_date'].dt.year < 2006].index, inplace=True)\n",
        "df.sort_values(by=['cid', 'id'], inplace=True, ignore_index=True)\n",
        "df.to_csv(\"revlog.csv\", index=False)\n",
        "print(\"revlog.csv saved!\")\n",
        "df = df[(df['type'] == 0) | (df['type'] == 1)].copy()\n",
        "df['real_days'] = df['review_date'] - timedelta(hours=next_day_starts_at)\n",
        "df['real_days'] = pd.DatetimeIndex(df['real_days'].dt.floor('D')).to_julian_date()\n",
        "df.drop_duplicates(['cid', 'real_days'], keep='first', inplace=True)\n",
        "df['delta_t'] = df.real_days.diff()\n",
        "df.dropna(inplace=True)\n",
        "df['delta_t'] = df['delta_t'].astype(dtype=int)\n",
        "df['i'] = 1\n",
        "df['r_history'] = \"\"\n",
        "df['t_history'] = \"\"\n",
        "col_idx = {key: i for i, key in enumerate(df.columns)}\n",
        "\n",
        "\n",
        "# code from https://github.com/L-M-Sherlock/anki_revlog_analysis/blob/main/revlog_analysis.py\n",
        "def get_feature(x):\n",
        "    for idx, log in enumerate(x.itertuples()):\n",
        "        if idx == 0:\n",
        "            x.iloc[idx, col_idx['delta_t']] = 0\n",
        "        if idx == x.shape[0] - 1:\n",
        "            break\n",
        "        x.iloc[idx + 1, col_idx['i']] = x.iloc[idx, col_idx['i']] + 1\n",
        "        x.iloc[idx + 1, col_idx['t_history']] = f\"{x.iloc[idx, col_idx['t_history']]},{x.iloc[idx, col_idx['delta_t']]}\"\n",
        "        x.iloc[idx + 1, col_idx['r_history']] = f\"{x.iloc[idx, col_idx['r_history']]},{x.iloc[idx, col_idx['r']]}\"\n",
        "    return x\n",
        "\n",
        "\n",
        "tqdm.pandas()\n",
        "df = df.groupby('cid', as_index=False).progress_apply(get_feature)\n",
        "df[\"t_history\"] = df[\"t_history\"].map(lambda x: x[1:] if len(x) > 1 else x)\n",
        "df[\"r_history\"] = df[\"r_history\"].map(lambda x: x[1:] if len(x) > 1 else x)\n",
        "df.to_csv('revlog_history.tsv', sep=\"\\t\", index=False)\n",
        "print(\"Trainset saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_SgzC-auWmu"
      },
      "source": [
        "## 2 Optimize parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrfBJjqCHEwJ"
      },
      "source": [
        "### 2.1 Define the model\n",
        "\n",
        "FSRS is a time-series model for predicting memory states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tdYp3GMLhTYm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "initStability = 1\n",
        "initStabilityRatingFactor = 1\n",
        "initDifficulty = 1\n",
        "initDifficultyRatingFactor = -1\n",
        "updateDifficultyRatingFactor = -1\n",
        "difficultyMeanReversionFactor = 0.2\n",
        "recallFactor = 3\n",
        "recallDifficultyDecay = -0.8\n",
        "recallStabilityDecay = -0.2\n",
        "recallRetrievabilityFactor = 1.3\n",
        "forgetFactor = 2.2\n",
        "forgetDifficultyDecay = -0.3\n",
        "forgetStabilityDecay = 0.3\n",
        "forgetRetrievabilityFactor = 1.2\n",
        "\n",
        "class FSRS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FSRS, self).__init__()\n",
        "        self.f_s = nn.Parameter(torch.FloatTensor([initStability, initStabilityRatingFactor]))\n",
        "        # init stability\n",
        "        self.f_d = nn.Parameter(torch.FloatTensor([initDifficulty, initDifficultyRatingFactor, updateDifficultyRatingFactor, difficultyMeanReversionFactor]))\n",
        "        # init difficulty\n",
        "        self.s_w = nn.Parameter(torch.FloatTensor([recallFactor, recallDifficultyDecay, recallStabilityDecay, recallRetrievabilityFactor, forgetFactor, forgetDifficultyDecay, forgetStabilityDecay, forgetRetrievabilityFactor]))\n",
        "        self.zero = torch.FloatTensor([0.0])\n",
        "\n",
        "    def forward(self, x, s, d):\n",
        "        '''\n",
        "        :param x: [review interval, review response]\n",
        "        :param s: stability\n",
        "        :param d: difficulty\n",
        "        :return:\n",
        "        '''\n",
        "        if torch.equal(s, self.zero):\n",
        "            # first learn, init memory states\n",
        "            new_d = self.f_d[0] * (self.f_d[1] * (x[1] - 4) + 1)\n",
        "            new_s = self.f_s[0] * (self.f_s[1] * (x[1] - 1) + 1)\n",
        "        else:\n",
        "            r = torch.exp(np.log(0.9) * x[0] / s)\n",
        "            new_d = d + self.f_d[2] * (x[1] - 3)\n",
        "            new_d = self.mean_reversion(self.f_d[0] * (- self.f_d[1] + 1), new_d)\n",
        "            new_d = self.constrain(new_d)\n",
        "            # recall\n",
        "            if x[1] > 1:\n",
        "                new_s = s * (1 + torch.exp(self.s_w[0]) * \n",
        "                            torch.pow(new_d, self.s_w[1]) *\n",
        "                            torch.pow(s, self.s_w[2]) *\n",
        "                            (torch.exp((1 - r) * self.s_w[3]) - 1))\n",
        "            # forget\n",
        "            else:\n",
        "                new_s = self.s_w[4] * torch.pow(new_d, self.s_w[5]) * torch.pow(s, self.s_w[6]) * torch.exp((1 - r) * self.s_w[7])\n",
        "        return new_s, new_d\n",
        "\n",
        "    def loss(self, s, t, r):\n",
        "        return - (r * np.log(0.9) * t / s + (1 - r) * torch.log(1 - torch.exp(np.log(0.9) * t / s)))\n",
        "\n",
        "    def constrain(self, d):\n",
        "        return torch.relu(d - 1) + 1\n",
        "\n",
        "    def mean_reversion(self, init, current):\n",
        "        return self.f_d[3] * init + (1-self.f_d[3]) * current\n",
        "\n",
        "\n",
        "class WeightClipper(object):\n",
        "    def __init__(self, frequency=1):\n",
        "        self.frequency = frequency\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if hasattr(module, 'f_s'):\n",
        "            w = module.f_s.data\n",
        "            w[0] = w[0].clamp(0.1, 10)  # initStability\n",
        "            w[1] = w[1].clamp(0.01, 10)  # initStabilityRatingFactor\n",
        "            module.f_s.data = w\n",
        "        if hasattr(module, 'f_d'):\n",
        "            w = module.f_d.data\n",
        "            w[0] = w[0].clamp(1, 10)  # initDifficulty\n",
        "            w[1] = w[1].clamp(-10, -0.01)  # initDifficultyRatingFactor\n",
        "            w[2] = w[2].clamp(-10, -0.01)  # updateDifficultyRatingFactor\n",
        "            w[3] = w[3].clamp(0, 1)  # difficultyMeanReversionFactor\n",
        "            module.f_d.data = w\n",
        "        if hasattr(module, 's_w'):\n",
        "            w = module.s_w.data\n",
        "            w[0] = w[0].clamp(0, 5)  # recallFactor\n",
        "            w[1] = w[1].clamp(-2, -0.01)  # recallDifficultyDecay\n",
        "            w[2] = w[2].clamp(-2, -0.01)  # recallStabilityDecay\n",
        "            w[3] = w[3].clamp(0.01, 2)  # recallRetrievabilityFactor\n",
        "            w[4] = w[4].clamp(0, 5)  # forgetFactor\n",
        "            w[5] = w[5].clamp(-2, -0.01)  # forgetDifficultyDecay\n",
        "            w[6] = w[6].clamp(0.01, 1)  # forgetStabilityDecay\n",
        "            w[7] = w[7].clamp(0.01, 2)  # forgetRetrievabilityFactor\n",
        "            module.s_w.data = w\n",
        "\n",
        "\n",
        "def lineToTensor(line):\n",
        "    ivl = line[0].split(',')\n",
        "    response = line[1].split(',')\n",
        "    tensor = torch.zeros(len(response), 2)\n",
        "    for li, response in enumerate(response):\n",
        "        tensor[li][0] = int(ivl[li])\n",
        "        tensor[li][1] = int(response)\n",
        "    return tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E1dYfgQLZAC"
      },
      "source": [
        "### 2.2 Train the model\n",
        "\n",
        "The `revlog_history.tsv` generated before will be used for training the FSRS model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jht0gneShowU",
        "outputId": "aaa72b79-b454-483b-d746-df1a353b2c8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56910/56910 [00:03<00:00, 15362.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorized!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:   0%|\u001b[31m          \u001b[0m| 26/56910 [00:00<03:40, 258.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 1\n",
            "f_s: [1.0005, 1.0005]\n",
            "f_d: [1.0, -0.9995, -0.9995, 0.2005]\n",
            "s_w: [3.0005, -0.7995, -0.1995, 1.3005, 2.2005, -0.2995, 0.3005, 1.2005]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  10%|\u001b[31m█         \u001b[0m| 5741/56910 [00:14<02:01, 421.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 5692\n",
            "f_s: [1.2702, 1.2586]\n",
            "f_d: [1.0024, -0.9651, -1.1192, 0.0521]\n",
            "s_w: [3.0059, -0.8937, -0.19, 1.3011, 2.2132, -0.2858, 0.334, 1.2064]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  20%|\u001b[31m██        \u001b[0m| 11434/56910 [00:28<02:02, 372.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 11383\n",
            "f_s: [1.4602, 1.4738]\n",
            "f_d: [1.0014, -0.9322, -1.1048, 0.0728]\n",
            "s_w: [3.0657, -0.8794, -0.1265, 1.3568, 2.2175, -0.2703, 0.3296, 1.1948]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  30%|\u001b[31m███       \u001b[0m| 17120/56910 [00:43<01:36, 410.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 17074\n",
            "f_s: [1.5726, 1.65]\n",
            "f_d: [1.0219, -0.9416, -1.1315, 0.0839]\n",
            "s_w: [3.0711, -0.9074, -0.0979, 1.3578, 2.1922, -0.2911, 0.3235, 1.1508]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  40%|\u001b[31m████      \u001b[0m| 22813/56910 [00:58<01:34, 359.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 22765\n",
            "f_s: [1.5704, 1.72]\n",
            "f_d: [1.0559, -0.9928, -1.1881, 0.061]\n",
            "s_w: [3.0357, -0.9603, -0.127, 1.3206, 2.1261, -0.3587, 0.2932, 1.0941]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  50%|\u001b[31m█████     \u001b[0m| 28533/56910 [01:12<01:13, 384.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 28456\n",
            "f_s: [1.6784, 1.8725]\n",
            "f_d: [1.0067, -0.9636, -1.1995, 0.0616]\n",
            "s_w: [3.0564, -0.9843, -0.0857, 1.3387, 2.1906, -0.2843, 0.3328, 1.1568]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  60%|\u001b[31m██████    \u001b[0m| 34228/56910 [01:26<00:53, 426.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 34147\n",
            "f_s: [1.6983, 1.8963]\n",
            "f_d: [1.0033, -0.9629, -1.2178, 0.0613]\n",
            "s_w: [3.0599, -1.0047, -0.0774, 1.3426, 2.1754, -0.286, 0.3243, 1.1376]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  70%|\u001b[31m███████   \u001b[0m| 39894/56910 [01:41<00:47, 357.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 39838\n",
            "f_s: [1.7047, 1.9418]\n",
            "f_d: [1.0155, -0.976, -1.1999, 0.1074]\n",
            "s_w: [3.0646, -1.0041, -0.0691, 1.3421, 2.1959, -0.2599, 0.3419, 1.141]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  80%|\u001b[31m████████  \u001b[0m| 45559/56910 [02:00<00:48, 236.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 45529\n",
            "f_s: [1.7054, 1.9824]\n",
            "f_d: [1.0233, -0.9943, -1.2296, 0.0866]\n",
            "s_w: [3.0414, -1.0547, -0.0734, 1.314, 2.2104, -0.2478, 0.3548, 1.1255]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  90%|\u001b[31m█████████ \u001b[0m| 51267/56910 [02:17<00:16, 338.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 51220\n",
            "f_s: [1.6519, 1.9887]\n",
            "f_d: [1.006, -0.9738, -1.2296, 0.0881]\n",
            "s_w: [3.062, -1.0546, -0.061, 1.3333, 2.2051, -0.2531, 0.3472, 1.1081]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|\u001b[31m██████████\u001b[0m| 56910/56910 [02:35<00:00, 365.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training finished!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = FSRS()\n",
        "clipper = WeightClipper()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "dataset = pd.read_csv(\"./revlog_history.tsv\", sep='\\t', index_col=None)\n",
        "dataset = dataset[(dataset['i'] > 1) & (dataset['delta_t'] > 0) & (dataset['t_history'].str.count(',0') == 0)]\n",
        "dataset['tensor'] = dataset.progress_apply(lambda x: lineToTensor(\n",
        "    list(zip([x['t_history']], [x['r_history']]))[0]), axis=1)\n",
        "print(\"Tensorized!\")\n",
        "\n",
        "\n",
        "pre_train_set = dataset[dataset['i'] == 2]\n",
        "# pretrain\n",
        "n_epoch = 5\n",
        "\n",
        "for k in range(n_epoch):\n",
        "    pre_train_set = shuffle(pre_train_set, random_state=2022 + k)\n",
        "    epoch_len = len(pre_train_set)\n",
        "    for i, (_, row) in enumerate(tqdm(pre_train_set.iterrows(), total=epoch_len, desc=\"train\", colour=\"red\")):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output_t = [(model.zero, model.zero)]\n",
        "        for input_t in row['tensor']:\n",
        "            output_t.append(model(input_t, *output_t[-1]))\n",
        "        loss = model.loss(output_t[-1][0], row['delta_t'],\n",
        "                            {1: 0, 2: 1, 3: 1, 4: 1}[row['r']])\n",
        "        if np.isnan(loss.data.item()):\n",
        "            # Exception Case\n",
        "            print(row, output_t)\n",
        "            raise Exception('error case')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.apply(clipper)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if name == \"f_s\":\n",
        "        param.requires_grad = False\n",
        "    print(f\"{name}: {list(map(lambda x: round(float(x), 4),param))}\")\n",
        "\n",
        "n_epoch = 1\n",
        "train_set = dataset[dataset['i'] > 2]\n",
        "print_len = max(train_set.shape[0] // 10, 1)\n",
        "\n",
        "for k in range(n_epoch):\n",
        "    train_set = shuffle(train_set, random_state=2022 + k)\n",
        "    epoch_len = len(train_set)\n",
        "    for i, (_, row) in enumerate(tqdm(train_set.iterrows(), total=epoch_len, desc=\"train\", colour=\"red\")):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output_t = [(model.zero, model.zero)]\n",
        "        for input_t in row['tensor']:\n",
        "            output_t.append(model(input_t, *output_t[-1]))\n",
        "        loss = model.loss(output_t[-1][0], row['delta_t'],\n",
        "                          {1: 0, 2: 1, 3: 1, 4: 1}[row['r']])\n",
        "        if np.isnan(loss.data.item()):\n",
        "            # Exception Case\n",
        "            print(row, output_t)\n",
        "            raise Exception('error case')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.apply(clipper)\n",
        "\n",
        "        if (k * epoch_len + i) % print_len == 0:\n",
        "            print(f\"iteration: {k * epoch_len + i + 1}\")\n",
        "            for name, param in model.named_parameters():\n",
        "                print(f\"{name}: {list(map(lambda x: round(float(x), 4),param))}\")\n",
        "\n",
        "\n",
        "initStability, initStabilityRatingFactor = map(\n",
        "    lambda x: round(float(x), 4), dict(model.named_parameters())['f_s'].data)\n",
        "initDifficulty, initDifficultyRatingFactor, updateDifficultyRatingFactor, difficultyMeanReversionFactor = map(\n",
        "    lambda x: round(float(x), 4), dict(model.named_parameters())['f_d'].data)\n",
        "recallFactor, recallDifficultyDecay, recallStabilityDecay, recallRetrievabilityFactor, forgetFactor, forgetDifficultyDecay, forgetStabilityDecay, forgetRetrievabilityFactor = map(\n",
        "    lambda x: round(float(x), 4), dict(model.named_parameters())['s_w'].data)\n",
        "\n",
        "print(\"\\nTraining finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ4S2l7BWfzr"
      },
      "source": [
        "## 3 Result\n",
        "\n",
        "Copy the optimal parameters for FSRS for you in the output of next code cell after running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTnPSDA2QpUu",
        "outputId": "49f487b9-69a7-4e96-b35a-7e027f478fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "var f_s = [1.6543,1.9849];\n",
            "var f_d = [1.006,-0.9394,-1.2311,0.0923];\n",
            "var s_w = [3.0788,-1.0484,-0.0571,1.3462,2.2089,-0.2442,0.3476,1.099];\n"
          ]
        }
      ],
      "source": [
        "print(f\"var f_s = [{initStability},{initStabilityRatingFactor}];\")\n",
        "print(f\"var f_d = [{initDifficulty},{initDifficultyRatingFactor},{updateDifficultyRatingFactor},{difficultyMeanReversionFactor}];\")\n",
        "print(f\"var s_w = [{recallFactor},{recallDifficultyDecay},{recallStabilityDecay},{recallRetrievabilityFactor},{forgetFactor},{forgetDifficultyDecay},{forgetStabilityDecay},{forgetRetrievabilityFactor}];\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_zsoDyTaTrT"
      },
      "source": [
        "You can see the memory states and intervals generated by FSRS as if you press the good in each review at the due date scheduled by FSRS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iws4rtP1WKBT",
        "outputId": "890d0287-1a17-4c59-fbbf-ee54d79cd383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1:again, 2:hard, 3:good, 4:easy\n",
            "\n",
            "first rating: 1\n",
            "rating history: 1,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,2,3,6,10,18,33,60,108,195,352,634,1138,2035,3622,6414\n",
            "difficulty history: 0,3.8,3.7,3.5,3.4,3.2,3.1,3.0,2.9,2.8,2.7,2.7,2.6,2.5,2.5,2.4\n",
            "\n",
            "first rating: 2\n",
            "rating history: 2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,5,10,19,37,72,139,266,504,949,1771,3275,6003,10900,19606,34928\n",
            "difficulty history: 0,2.9,2.8,2.7,2.7,2.6,2.5,2.5,2.4,2.4,2.3,2.3,2.3,2.2,2.2,2.2\n",
            "\n",
            "first rating: 3\n",
            "rating history: 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,8,19,44,99,218,467,978,2004,4022,7914,15282,28985,54043,99131,179022\n",
            "difficulty history: 0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0\n",
            "\n",
            "first rating: 4\n",
            "rating history: 4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,12,41,129,373,1001,2521,6010,13650,29702,62211,125918,247134,471701,877783,1596050\n",
            "difficulty history: 0,1.0,1.1,1.2,1.2,1.3,1.4,1.4,1.5,1.5,1.6,1.6,1.6,1.7,1.7,1.7\n",
            "\n"
          ]
        }
      ],
      "source": [
        "requestRetention = 0.9  # recommended setting: 0.8 ~ 0.9\n",
        "\n",
        "\n",
        "class Collection:\n",
        "    def __init__(self):\n",
        "        self.model = model\n",
        "\n",
        "    def states(self, t_history, r_history):\n",
        "        with torch.no_grad():\n",
        "            line_tensor = lineToTensor(list(zip([t_history], [r_history]))[0])\n",
        "            output_t = [(self.model.zero, self.model.zero)]\n",
        "            for input_t in line_tensor:\n",
        "                output_t.append(self.model(input_t, *output_t[-1]))\n",
        "            return output_t[-1]\n",
        "\n",
        "\n",
        "my_collection = Collection()\n",
        "print(\"1:again, 2:hard, 3:good, 4:easy\\n\")\n",
        "for first_rating in (1,2,3,4):\n",
        "    print(f'first rating: {first_rating}')\n",
        "    t_history = \"0\"\n",
        "    d_history = \"0\"\n",
        "    r_history = f\"{first_rating}\"  # the first rating of the new card\n",
        "    # print(\"stability, difficulty, lapses\")\n",
        "    for i in range(15):\n",
        "        states = my_collection.states(t_history, r_history)\n",
        "        # print('{0:9.2f} {1:11.2f} {2:7.0f}'.format(\n",
        "            # *list(map(lambda x: round(float(x), 4), states))))\n",
        "        next_t = round(float(np.log(requestRetention)/np.log(0.9) * states[0]))\n",
        "        difficulty = round(float(states[1]), 1)\n",
        "        t_history += f',{int(next_t)}'\n",
        "        d_history += f',{difficulty}'\n",
        "        r_history += f\",3\"\n",
        "    print(f\"rating history: {r_history}\")\n",
        "    print(f\"interval history: {t_history}\")\n",
        "    print(f\"difficulty history: {d_history}\")\n",
        "    print('')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can change the `test_rating_sequence` to see the scheduling intervals in different ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor(8.2213), tensor(1.9510))\n",
            "(tensor(3.6043), tensor(4.1858))\n",
            "(tensor(2.4923), tensor(6.2143))\n",
            "(tensor(3.4277), tensor(5.8206))\n",
            "(tensor(4.9015), tensor(5.4632))\n",
            "(tensor(7.4729), tensor(5.1389))\n",
            "(tensor(11.2073), tensor(4.8445))\n",
            "(tensor(3.5671), tensor(6.8121))\n",
            "(tensor(5.2437), tensor(6.3632))\n",
            "(tensor(7.4362), tensor(5.9558))\n",
            "(tensor(10.6537), tensor(5.5859))\n",
            "(tensor(15.9462), tensor(5.2503))\n",
            "(tensor(23.9526), tensor(4.9456))\n",
            "(tensor(36.4160), tensor(4.6690))\n",
            "(tensor(55.7545), tensor(4.4180))\n",
            "(tensor(86.7965), tensor(4.1902))\n",
            "(tensor(7.4459), tensor(6.2183))\n",
            "rating history: 3,1,1,3,3,3,3,1,3,3,3,3,3,3,3,3,1\n",
            "interval history: 0,8,4,2,3,5,7,11,4,5,7,11,16,24,36,56,87,7\n",
            "difficulty history: 0,2.0,4.2,6.2,5.8,5.5,5.1,4.8,6.8,6.4,6.0,5.6,5.3,4.9,4.7,4.4,4.2,6.2\n"
          ]
        }
      ],
      "source": [
        "test_rating_sequence = \"3,1,1,3,3,3,3,1,3,3,3,3,3,3,3,3,1\"\n",
        "requestRetention = 0.9  # recommended setting: 0.8 ~ 0.9\n",
        "easyBonus = 1.3\n",
        "hardInterval = 1.2\n",
        "\n",
        "t_history = \"0\"\n",
        "d_history = \"0\"\n",
        "for i in range(len(test_rating_sequence.split(','))):\n",
        "    rating = test_rating_sequence[2*i]\n",
        "    last_t = int(t_history.split(',')[-1])\n",
        "    r_history = test_rating_sequence[:2*i+1]\n",
        "    states = my_collection.states(t_history, r_history)\n",
        "    print(states)\n",
        "    next_t = max(1,round(float(np.log(requestRetention)/np.log(0.9) * states[0])))\n",
        "    if rating == '4':\n",
        "        next_t = round(next_t * easyBonus)\n",
        "    elif rating == '2':\n",
        "        next_t = round(last_t * hardInterval)\n",
        "    t_history += f',{int(next_t)}'\n",
        "    difficulty = round(float(np.log(requestRetention)/np.log(0.9) * states[1]), 1)\n",
        "    d_history += f',{difficulty}'\n",
        "print(f\"rating history: {test_rating_sequence}\")\n",
        "print(f\"interval history: {t_history}\")\n",
        "print(f\"difficulty history: {d_history}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMnk8/Ih2JAJZJ1PBkXQUBC",
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('fsrs4anki')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "8dd9a290ffd10997e0b0d411ff1325a47862ea932e0fd309ade800e0e51d2b4b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
