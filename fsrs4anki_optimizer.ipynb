{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FSRS4Anki v3.2.0 Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurCmW0Jqz3s"
      },
      "source": [
        "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/open-spaced-repetition/fsrs4anki/blob/v3.2.0/fsrs4anki_optimizer.ipynb)\n",
        "\n",
        "↑ Click the above button to open the optimizer on Google Colab.\n",
        "\n",
        "> If you can't see the button and are located in the Chinese Mainland, please use a proxy or VPN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG7bBfGJFbMr"
      },
      "source": [
        "Upload your **Anki Deck Package (.apkg)** file or **Anki Collection Package (.colpkg)** file on the `Left sidebar -> Files`, drag and drop your file in the current directory (not the `sample_data` directory). \n",
        "\n",
        "No need to include media. Need to include scheduling information. \n",
        "\n",
        "> If you use the latest version of Anki, please check the box `Support older Anki versions (slower/larger files)` when you export.\n",
        "\n",
        "You can export it via `File -> Export...` or `Ctrl + E` in the main window of Anki.\n",
        "\n",
        "Then replace the `filename` with yours in the next code cell. And set the `timezone` and `next_day_starts_at` which can be found in your preferences of Anki.\n",
        "\n",
        "After that, just run all (`Runtime -> Run all` or `Ctrl + F9`) and wait for minutes. You can see the optimal parameters in section **3 Result**. Copy them, replace the parameters in `fsrs4anki_scheduler.js`, and paste them into the custom scheduling of your deck options (require Anki version >= 2.1.55).\n",
        "\n",
        "**NOTE**: The default output is generated from my review logs. If you find the output is the same as mine, maybe your notebook hasn't run there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iqP70_-3EUhi"
      },
      "outputs": [],
      "source": [
        "# Here are some settings that you need to replace before running this optimizer.\n",
        "\n",
        "filename = \"collection-2022-09-18@13-21-58.colpkg\"\n",
        "# If you upload deck file, replace it with your deck filename. E.g., ALL__Learning.apkg\n",
        "# If you upload collection file, replace it with your colpgk filename. E.g., collection-2022-09-18@13-21-58.colpkg\n",
        "\n",
        "# Replace it with your timezone. I'm in China, so I use Asia/Shanghai.\n",
        "timezone = 'Asia/Shanghai'\n",
        "\n",
        "# Replace it with your Anki's setting in Prefernces -> Scheduling.\n",
        "next_day_starts_at = 4\n",
        "\n",
        "# Replace it if you don't want the optimizer to use the review logs before a specific date.\n",
        "revlog_start_date = \"2006-10-05\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLFVNmG2qd06"
      },
      "source": [
        "## 1 Build dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkzFeKawqgbs"
      },
      "source": [
        "### 1.1 Extract Anki collection & deck file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD2js_wEr_Bs",
        "outputId": "42653d9e-316e-40bc-bd1d-f3a0e2b246c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extract successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "# Extract the collection file or deck file to get the .anki21 database.\n",
        "with zipfile.ZipFile(f'./{filename}', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n",
        "    print(\"Extract successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKpy4VfqGmaL"
      },
      "source": [
        "### 1.2 Create time-series feature\n",
        "\n",
        "The following code cell will extract the review logs from your Anki collection and preprocess them to a trainset which is saved in `revlog_history.tsv`.\n",
        "\n",
        " The time-series features are important in optimizing the model's parameters. For more detail, please see my paper: https://www.maimemo.com/paper/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2IIaY3PDaaG",
        "outputId": "607916c9-da95-48dd-fdab-6bd83fbbbb40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "revlog.csv saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30711/30711 [01:10<00:00, 435.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 96660/96660 [00:44<00:00, 2173.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retention calculated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1312/1312 [00:01<00:00, 899.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stability calculated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1312/1312 [00:00<00:00, 1508.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis saved!\n",
            "       r_history  avg_interval  avg_retention  stability  factor  group_cnt\n",
            "1              1           1.7         0.7649     1.0391     inf       7978\n",
            "2              2           1.0         0.9009     1.0893     inf        234\n",
            "3              3           1.5         0.9622     5.3588     inf       9070\n",
            "4              4           3.8         0.9656    12.1251     inf      11436\n",
            "12           3,1           1.1         0.9260     1.5290  0.2853        410\n",
            "13           3,2           3.6         0.9296     8.3895  1.5656       1091\n",
            "14           3,3           3.9         0.9664    15.1840  2.8335       6527\n",
            "15           3,4           8.8         0.9376    20.8551  3.8917        724\n",
            "45         3,3,1           1.2         0.9408     2.2464  0.1479        239\n",
            "46         3,3,2           6.5         0.9319    16.7267  1.1016        594\n",
            "47         3,3,3           9.0         0.9602    23.5304  1.5497       5036\n",
            "152      3,3,3,1           1.8         0.9460     3.5427  0.1506        357\n",
            "153      3,3,3,2          22.8         0.8804    18.4659  0.7848        448\n",
            "154      3,3,3,3          18.6         0.9412    35.1949  1.4957       3052\n",
            "369    3,3,3,3,1           1.5         0.9519     3.9697  0.1128        249\n",
            "370    3,3,3,3,2          23.3         0.8438    14.4558  0.4107        149\n",
            "371    3,3,3,3,3          39.5         0.9135    46.9134  1.3330       1423\n",
            "667  3,3,3,3,3,3          74.3         0.8528    55.5966  1.1851        411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import time\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import timedelta, datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "if os.path.isfile(\"collection.anki21b\"):\n",
        "    os.remove(\"collection.anki21b\")\n",
        "    raise Exception(\n",
        "        \"Please export the file with `support older Anki versions` if you use the latest version of Anki.\")\n",
        "elif os.path.isfile(\"collection.anki21\"):\n",
        "    con = sqlite3.connect(\"collection.anki21\")\n",
        "elif os.path.isfile(\"collection.anki2\"):\n",
        "    con = sqlite3.connect(\"collection.anki2\")\n",
        "else:\n",
        "    raise Exception(\"Collection not exist!\")\n",
        "cur = con.cursor()\n",
        "res = cur.execute(\"SELECT * FROM revlog\")\n",
        "revlog = res.fetchall()\n",
        "\n",
        "df = pd.DataFrame(revlog)\n",
        "df.columns = ['id', 'cid', 'usn', 'r', 'ivl',\n",
        "              'last_lvl', 'factor', 'time', 'type']\n",
        "df = df[(df['cid'] <= time.time() * 1000) &\n",
        "        (df['id'] <= time.time() * 1000) &\n",
        "        (df['id'] >= time.mktime(datetime.strptime(revlog_start_date, \"%Y-%m-%d\").timetuple()) * 1000)].copy()\n",
        "df['create_date'] = pd.to_datetime(df['cid'] // 1000, unit='s')\n",
        "df['create_date'] = df['create_date'].dt.tz_localize(\n",
        "    'UTC').dt.tz_convert(timezone)\n",
        "df['review_date'] = pd.to_datetime(df['id'] // 1000, unit='s')\n",
        "df['review_date'] = df['review_date'].dt.tz_localize(\n",
        "    'UTC').dt.tz_convert(timezone)\n",
        "df.drop(df[df['review_date'].dt.year < 2006].index, inplace=True)\n",
        "df.sort_values(by=['cid', 'id'], inplace=True, ignore_index=True)\n",
        "df.to_csv(\"revlog.csv\", index=False)\n",
        "print(\"revlog.csv saved.\")\n",
        "df = df[(df['type'] == 0) | (df['type'] == 1)].copy()\n",
        "df['real_days'] = df['review_date'] - timedelta(hours=next_day_starts_at)\n",
        "df['real_days'] = pd.DatetimeIndex(df['real_days'].dt.floor('D')).to_julian_date()\n",
        "df.drop_duplicates(['cid', 'real_days'], keep='first', inplace=True)\n",
        "df['delta_t'] = df.real_days.diff()\n",
        "df.dropna(inplace=True)\n",
        "df['delta_t'] = df['delta_t'].astype(dtype=int)\n",
        "df['i'] = 1\n",
        "df['r_history'] = \"\"\n",
        "df['t_history'] = \"\"\n",
        "col_idx = {key: i for i, key in enumerate(df.columns)}\n",
        "\n",
        "\n",
        "# code from https://github.com/L-M-Sherlock/anki_revlog_analysis/blob/main/revlog_analysis.py\n",
        "def get_feature(x):\n",
        "    for idx, log in enumerate(x.itertuples()):\n",
        "        if idx == 0:\n",
        "            x.iloc[idx, col_idx['delta_t']] = 0\n",
        "        if idx == x.shape[0] - 1:\n",
        "            break\n",
        "        x.iloc[idx + 1, col_idx['i']] = x.iloc[idx, col_idx['i']] + 1\n",
        "        x.iloc[idx + 1, col_idx['t_history']] = f\"{x.iloc[idx, col_idx['t_history']]},{x.iloc[idx, col_idx['delta_t']]}\"\n",
        "        x.iloc[idx + 1, col_idx['r_history']] = f\"{x.iloc[idx, col_idx['r_history']]},{x.iloc[idx, col_idx['r']]}\"\n",
        "    return x\n",
        "\n",
        "\n",
        "tqdm.pandas()\n",
        "df = df.groupby('cid', as_index=False).progress_apply(get_feature)\n",
        "df[\"t_history\"] = df[\"t_history\"].map(lambda x: x[1:] if len(x) > 1 else x)\n",
        "df[\"r_history\"] = df[\"r_history\"].map(lambda x: x[1:] if len(x) > 1 else x)\n",
        "df.to_csv('revlog_history.tsv', sep=\"\\t\", index=False)\n",
        "print(\"Trainset saved.\")\n",
        "\n",
        "def cal_retention(group: pd.DataFrame) -> pd.DataFrame:\n",
        "    group['retention'] = round(group['r'].map(lambda x: {1: 0, 2: 1, 3: 1, 4: 1}[x]).mean(), 4)\n",
        "    group['total_cnt'] = group.shape[0]\n",
        "    return group\n",
        "\n",
        "df = df.groupby(by=['r_history', 'delta_t']).progress_apply(cal_retention)\n",
        "print(\"Retention calculated.\")\n",
        "df = df.drop(columns=['id', 'cid', 'usn', 'ivl', 'last_lvl', 'factor', 'time', 'type', 'create_date', 'review_date', 'real_days', 'r', 't_history'])\n",
        "df.drop_duplicates(inplace=True)\n",
        "df = df[(df['retention'] < 1) & (df['retention'] > 0)]\n",
        "\n",
        "def cal_stability(group: pd.DataFrame) -> pd.DataFrame:\n",
        "    if group['i'].values[0] > 1:\n",
        "        r_ivl_cnt = sum(group['delta_t'] * group['retention'].map(np.log) * pow(group['total_cnt'], 2))\n",
        "        ivl_ivl_cnt = sum(group['delta_t'].map(lambda x: x ** 2) * pow(group['total_cnt'], 2))\n",
        "        group['stability'] = round(np.log(0.9) / (r_ivl_cnt / ivl_ivl_cnt), 4)\n",
        "    else:\n",
        "        group['stability'] = 0.0\n",
        "    group['group_cnt'] = sum(group['total_cnt'])\n",
        "    group['avg_retention'] = round(sum(group['retention'] * pow(group['total_cnt'], 2)) / sum(pow(group['total_cnt'], 2)), 4)\n",
        "    group['avg_interval'] = round(sum(group['delta_t'] * pow(group['total_cnt'], 2)) / sum(pow(group['total_cnt'], 2)), 1)\n",
        "    del group['total_cnt']\n",
        "    del group['retention']\n",
        "    del group['delta_t']\n",
        "    return group\n",
        "\n",
        "df = df.groupby(by=['r_history']).progress_apply(cal_stability)\n",
        "print(\"Stability calculated.\")\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.sort_values(by=['i', 'r_history'], inplace=True, ignore_index=True)\n",
        "\n",
        "for idx in tqdm(df.index):\n",
        "    item = df.loc[idx]\n",
        "    index = df[(df['i'] == item['i'] + 1) & (df['r_history'].str.startswith(item['r_history']))].index\n",
        "    df.loc[index, 'last_stability'] = item['stability']\n",
        "df['factor'] = round(df['stability'] / df['last_stability'], 4)\n",
        "df = df[(df['i'] >= 2) & (df['group_cnt'] >= 100)]\n",
        "df['last_recall'] = df['r_history'].map(lambda x: x[-1])\n",
        "df = df[df.groupby(['i', 'r_history'])['group_cnt'].transform(max) == df['group_cnt']]\n",
        "df.to_csv('./stability_for_analysis.tsv', sep='\\t', index=None)\n",
        "print(\"Analysis saved!\")\n",
        "print(df[~df['r_history'].str.contains(r'[^3],', regex=True)][['r_history', 'avg_interval', 'avg_retention', 'stability', 'factor', 'group_cnt']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_SgzC-auWmu"
      },
      "source": [
        "## 2 Optimize parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrfBJjqCHEwJ"
      },
      "source": [
        "### 2.1 Define the model\n",
        "\n",
        "FSRS is a time-series model for predicting memory states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tdYp3GMLhTYm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "init_w = [1, 1, 5, -0.5, -0.5, 0.2, 1.4, -0.02, 0.8, 2, -0.2, 0.5, 1]\n",
        "\n",
        "\n",
        "class FSRS(nn.Module):\n",
        "    def __init__(self, w):\n",
        "        super(FSRS, self).__init__()\n",
        "        self.w = nn.Parameter(torch.FloatTensor(w))\n",
        "        self.zero = torch.FloatTensor([0.0])\n",
        "\n",
        "    def forward(self, x, s, d):\n",
        "        '''\n",
        "        :param x: [review interval, review response]\n",
        "        :param s: stability\n",
        "        :param d: difficulty\n",
        "        :return:\n",
        "        '''\n",
        "        if torch.equal(s, self.zero):\n",
        "            # first learn, init memory states\n",
        "            new_s = self.w[0] + self.w[1] * (x[1] - 1)\n",
        "            new_d = self.w[2] + self.w[3] * (x[1] - 3)\n",
        "            new_d = new_d.clamp(1, 10)\n",
        "        else:\n",
        "            r = torch.exp(np.log(0.9) * x[0] / s)\n",
        "            new_d = d + self.w[4] * (x[1] - 3)\n",
        "            new_d = self.mean_reversion(self.w[2], new_d)\n",
        "            new_d = new_d.clamp(1, 10)\n",
        "            # recall\n",
        "            if x[1] > 1:\n",
        "                new_s = s * (1 + torch.exp(self.w[6]) *\n",
        "                             (11 - new_d) *\n",
        "                             torch.pow(s, self.w[7]) *\n",
        "                             (torch.exp((1 - r) * self.w[8]) - 1))\n",
        "            # forget\n",
        "            else:\n",
        "                new_s = self.w[9] * torch.pow(new_d, self.w[10]) * torch.pow(\n",
        "                    s, self.w[11]) * torch.exp((1 - r) * self.w[12])\n",
        "        return new_s, new_d\n",
        "\n",
        "    def loss(self, s, t, r):\n",
        "        return - (r * np.log(0.9) * t / s + (1 - r) * torch.log(1 - torch.exp(np.log(0.9) * t / s)))\n",
        "\n",
        "    def mean_reversion(self, init, current):\n",
        "        return self.w[5] * init + (1-self.w[5]) * current\n",
        "\n",
        "\n",
        "class WeightClipper(object):\n",
        "    def __init__(self, frequency=1):\n",
        "        self.frequency = frequency\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if hasattr(module, 'w'):\n",
        "            w = module.w.data\n",
        "            w[0] = w[0].clamp(0.1, 10)  # initStability\n",
        "            w[1] = w[1].clamp(0.1, 5)  # initStabilityRatingFactor\n",
        "            w[2] = w[2].clamp(1, 10)  # initDifficulty\n",
        "            w[3] = w[3].clamp(-5, -0.1)  # initDifficultyRatingFactor\n",
        "            w[4] = w[4].clamp(-5, -0.1)  # updateDifficultyRatingFactor\n",
        "            w[5] = w[5].clamp(0, 0.5)  # difficultyMeanReversionFactor\n",
        "            w[6] = w[6].clamp(0, 5)  # recallFactor\n",
        "            w[7] = w[7].clamp(-0.2, -0.01)  # recallStabilityDecay\n",
        "            w[8] = w[8].clamp(0.01, 2)  # recallRetrievabilityFactor\n",
        "            w[9] = w[9].clamp(0.5, 5)  # forgetFactor\n",
        "            w[10] = w[10].clamp(-2, -0.01)  # forgetDifficultyDecay\n",
        "            w[11] = w[11].clamp(0.01, 1)  # forgetStabilityDecay\n",
        "            w[12] = w[12].clamp(0.01, 2)  # forgetRetrievabilityFactor\n",
        "            module.w.data = w\n",
        "\n",
        "def lineToTensor(line):\n",
        "    ivl = line[0].split(',')\n",
        "    response = line[1].split(',')\n",
        "    tensor = torch.zeros(len(response), 2)\n",
        "    for li, response in enumerate(response):\n",
        "        tensor[li][0] = int(ivl[li])\n",
        "        tensor[li][1] = int(response)\n",
        "    return tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E1dYfgQLZAC"
      },
      "source": [
        "### 2.2 Train the model\n",
        "\n",
        "The `revlog_history.tsv` generated before will be used for training the FSRS model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jht0gneShowU",
        "outputId": "aaa72b79-b454-483b-d746-df1a353b2c8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 225934/225934 [00:11<00:00, 19336.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorized!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pre—train: 100%|\u001b[31m██████████\u001b[0m| 28972/28972 [00:07<00:00, 3776.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w: [1.0138, 2.293, 5.0, -0.5, -0.5, 0.2, 1.4, -0.02, 0.8, 2.0, -0.2, 0.5, 1.0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:   0%|\u001b[31m          \u001b[0m| 11/196962 [00:00<29:51, 109.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 1\n",
            "w: [1.0138, 2.293, 4.9984, -0.4984, -0.4984, 0.2016, 1.4016, -0.0184, 0.8016, 2.0016, -0.1984, 0.5016, 1.0016]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  10%|\u001b[31m█         \u001b[0m| 19784/196962 [00:37<05:33, 530.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 19697\n",
            "w: [1.014, 2.2933, 5.1859, -0.8665, -0.753, 0.0278, 1.3077, -0.0323, 0.6998, 1.783, -0.4174, 0.4751, 0.7849]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  20%|\u001b[31m██        \u001b[0m| 39496/196962 [01:13<04:44, 553.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 39393\n",
            "w: [1.014, 2.2933, 5.1883, -1.0109, -0.8103, 0.0189, 1.3359, -0.0402, 0.722, 1.7791, -0.4218, 0.5329, 0.7656]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  30%|\u001b[31m███       \u001b[0m| 59190/196962 [01:49<04:42, 488.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 59089\n",
            "w: [1.014, 2.2933, 5.1806, -1.0338, -0.9271, 0.009, 1.3561, -0.0459, 0.734, 1.7582, -0.4392, 0.596, 0.8001]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  40%|\u001b[31m████      \u001b[0m| 78870/196962 [02:27<03:50, 513.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 78785\n",
            "w: [1.014, 2.2933, 5.231, -1.0983, -1.0567, 0.022, 1.4053, -0.0366, 0.7799, 1.7125, -0.4748, 0.6083, 0.7739]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  50%|\u001b[31m█████     \u001b[0m| 98547/196962 [03:06<03:13, 509.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 98481\n",
            "w: [1.014, 2.2933, 5.1966, -1.0786, -1.1153, 0.005, 1.4282, -0.0441, 0.7988, 1.7388, -0.4457, 0.6353, 0.8087]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  60%|\u001b[31m██████    \u001b[0m| 118263/196962 [03:44<02:34, 508.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 118177\n",
            "w: [1.014, 2.2933, 5.1881, -1.1304, -1.1087, 0.0283, 1.4025, -0.0234, 0.7683, 1.7037, -0.4725, 0.5728, 0.8558]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  70%|\u001b[31m███████   \u001b[0m| 137986/196962 [04:22<01:43, 567.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 137873\n",
            "w: [1.014, 2.2933, 5.146, -1.1969, -1.0858, 0.0077, 1.388, -0.026, 0.746, 1.7052, -0.4699, 0.6312, 0.9203]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  80%|\u001b[31m████████  \u001b[0m| 157658/196962 [05:02<01:14, 530.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 157569\n",
            "w: [1.014, 2.2933, 5.1197, -1.1527, -1.0874, 0.0259, 1.368, -0.0762, 0.7209, 1.7295, -0.4381, 0.6247, 0.9711]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:  90%|\u001b[31m█████████ \u001b[0m| 177330/196962 [05:39<00:36, 541.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 177265\n",
            "w: [1.014, 2.2933, 5.0225, -1.1358, -1.0742, 0.0118, 1.4074, -0.0451, 0.7556, 1.7049, -0.461, 0.5968, 0.994]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|\u001b[31m██████████\u001b[0m| 196962/196962 [06:18<00:00, 519.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 196961\n",
            "w: [1.014, 2.2933, 4.9588, -1.1608, -0.9955, 0.0235, 1.3923, -0.0485, 0.7363, 1.6938, -0.4707, 0.6032, 0.9763]\n",
            "\n",
            "Training finished!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = FSRS(init_w)\n",
        "clipper = WeightClipper()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "dataset = pd.read_csv(\"./revlog_history.tsv\", sep='\\t', index_col=None)\n",
        "dataset = dataset[(dataset['i'] > 1) & (dataset['delta_t'] > 0) & (dataset['t_history'].str.count(',0') == 0)]\n",
        "dataset['tensor'] = dataset.progress_apply(lambda x: lineToTensor(\n",
        "    list(zip([x['t_history']], [x['r_history']]))[0]), axis=1)\n",
        "print(\"Tensorized!\")\n",
        "\n",
        "pre_train_set = dataset[dataset['i'] == 2]\n",
        "# pretrain\n",
        "epoch_len = len(pre_train_set)\n",
        "n_epoch = 1\n",
        "pbar = tqdm(desc=\"pre—train\", colour=\"red\", total=epoch_len*n_epoch)\n",
        "\n",
        "for k in range(n_epoch):\n",
        "    for i, (_, row) in enumerate(shuffle(pre_train_set, random_state=2022 + k).iterrows()):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output_t = [(model.zero, model.zero)]\n",
        "        for input_t in row['tensor']:\n",
        "            output_t.append(model(input_t, *output_t[-1]))\n",
        "        loss = model.loss(output_t[-1][0], row['delta_t'],\n",
        "                            {1: 0, 2: 1, 3: 1, 4: 1}[row['r']])\n",
        "        if np.isnan(loss.data.item()):\n",
        "            # Exception Case\n",
        "            print(row, output_t)\n",
        "            raise Exception('error case')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.apply(clipper)\n",
        "        pbar.update()\n",
        "pbar.close()\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {list(map(lambda x: round(float(x), 4),param))}\")\n",
        "\n",
        "train_set = dataset[dataset['i'] > 2]\n",
        "epoch_len = len(train_set)\n",
        "n_epoch = 1\n",
        "print_len = max(epoch_len*n_epoch // 10, 1)\n",
        "pbar = tqdm(desc=\"train\", colour=\"red\", total=epoch_len*n_epoch)\n",
        "\n",
        "for k in range(n_epoch):\n",
        "    for i, (_, row) in enumerate(shuffle(train_set, random_state=2022 + k).iterrows()):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output_t = [(model.zero, model.zero)]\n",
        "        for input_t in row['tensor']:\n",
        "            output_t.append(model(input_t, *output_t[-1]))\n",
        "        loss = model.loss(output_t[-1][0], row['delta_t'],\n",
        "                          {1: 0, 2: 1, 3: 1, 4: 1}[row['r']])\n",
        "        if np.isnan(loss.data.item()):\n",
        "            # Exception Case\n",
        "            print(row, output_t)\n",
        "            raise Exception('error case')\n",
        "        loss.backward()\n",
        "        for param in model.parameters():\n",
        "            param.grad[:2] = torch.zeros(2)\n",
        "        optimizer.step()\n",
        "        model.apply(clipper)\n",
        "        pbar.update()\n",
        "\n",
        "        if (k * epoch_len + i) % print_len == 0:\n",
        "            print(f\"iteration: {k * epoch_len + i + 1}\")\n",
        "            for name, param in model.named_parameters():\n",
        "                print(f\"{name}: {list(map(lambda x: round(float(x), 4),param))}\")\n",
        "pbar.close()\n",
        "\n",
        "w = list(map(lambda x: round(float(x), 4), dict(model.named_parameters())['w'].data))\n",
        "\n",
        "print(\"\\nTraining finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ4S2l7BWfzr"
      },
      "source": [
        "## 3 Result\n",
        "\n",
        "Copy the optimal parameters for FSRS for you in the output of next code cell after running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTnPSDA2QpUu",
        "outputId": "49f487b9-69a7-4e96-b35a-7e027f478fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "var w = [1.014, 2.2933, 4.9588, -1.1608, -0.9954, 0.0234, 1.3923, -0.0484, 0.7363, 1.6937, -0.4708, 0.6032, 0.9762];\n"
          ]
        }
      ],
      "source": [
        "print(f\"var w = {w};\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_zsoDyTaTrT"
      },
      "source": [
        "You can see the memory states and intervals generated by FSRS as if you press the good in each review at the due date scheduled by FSRS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iws4rtP1WKBT",
        "outputId": "890d0287-1a17-4c59-fbbf-ee54d79cd383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1:again, 2:hard, 3:good, 4:easy\n",
            "\n",
            "first rating: 1\n",
            "rating history: 1,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,1,2,4,9,19,39,79,159,317,624\n",
            "difficulty history: 0,7.3,7.2,7.2,7.1,7.1,7.0,7.0,6.9,6.9,6.8\n",
            "\n",
            "first rating: 2\n",
            "rating history: 2,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,3,8,19,44,100,223,489,1052,2226,4631\n",
            "difficulty history: 0,6.1,6.1,6.1,6.0,6.0,6.0,6.0,5.9,5.9,5.9\n",
            "\n",
            "first rating: 3\n",
            "rating history: 3,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,6,16,42,107,265,641,1512,3483,7842,17280\n",
            "difficulty history: 0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0\n",
            "\n",
            "first rating: 4\n",
            "rating history: 4,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,8,24,69,192,517,1348,3409,8376,20022,46625\n",
            "difficulty history: 0,3.8,3.8,3.9,3.9,3.9,3.9,4.0,4.0,4.0,4.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "requestRetention = 0.9  # recommended setting: 0.8 ~ 0.9\n",
        "\n",
        "\n",
        "class Collection:\n",
        "    def __init__(self):\n",
        "        self.model = model\n",
        "\n",
        "    def states(self, t_history, r_history):\n",
        "        with torch.no_grad():\n",
        "            line_tensor = lineToTensor(list(zip([t_history], [r_history]))[0])\n",
        "            output_t = [(self.model.zero, self.model.zero)]\n",
        "            for input_t in line_tensor:\n",
        "                output_t.append(self.model(input_t, *output_t[-1]))\n",
        "            return output_t[-1]\n",
        "\n",
        "\n",
        "my_collection = Collection()\n",
        "print(\"1:again, 2:hard, 3:good, 4:easy\\n\")\n",
        "for first_rating in (1,2,3,4):\n",
        "    print(f'first rating: {first_rating}')\n",
        "    t_history = \"0\"\n",
        "    d_history = \"0\"\n",
        "    r_history = f\"{first_rating}\"  # the first rating of the new card\n",
        "    # print(\"stability, difficulty, lapses\")\n",
        "    for i in range(10):\n",
        "        states = my_collection.states(t_history, r_history)\n",
        "        # print('{0:9.2f} {1:11.2f} {2:7.0f}'.format(\n",
        "            # *list(map(lambda x: round(float(x), 4), states))))\n",
        "        next_t = max(round(float(np.log(requestRetention)/np.log(0.9) * states[0])), 1)\n",
        "        difficulty = round(float(states[1]), 1)\n",
        "        t_history += f',{int(next_t)}'\n",
        "        d_history += f',{difficulty}'\n",
        "        r_history += f\",3\"\n",
        "    print(f\"rating history: {r_history}\")\n",
        "    print(f\"interval history: {t_history}\")\n",
        "    print(f\"difficulty history: {d_history}\")\n",
        "    print('')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can change the `test_rating_sequence` to see the scheduling intervals in different ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor(5.6006), tensor(4.9588))\n",
            "(tensor(15.8415), tensor(4.9588))\n",
            "(tensor(41.8368), tensor(4.9588))\n",
            "(tensor(106.9482), tensor(4.9588))\n",
            "(tensor(265.4701), tensor(4.9588))\n",
            "(tensor(21.7953), tensor(6.9029))\n",
            "(tensor(4.3077), tensor(8.8015))\n",
            "(tensor(6.9330), tensor(8.7116))\n",
            "(tensor(11.5889), tensor(8.6238))\n",
            "(tensor(19.6520), tensor(8.5380))\n",
            "(tensor(33.2010), tensor(8.4543))\n",
            "(tensor(55.7056), tensor(8.3725))\n",
            "rating history: 3,3,3,3,3,1,1,3,3,3,3,3\n",
            "interval history: 0,6,16,42,107,265,22,4,7,12,20,33,56\n",
            "difficulty history: 0,5.0,5.0,5.0,5.0,5.0,6.9,8.8,8.7,8.6,8.5,8.5,8.4\n"
          ]
        }
      ],
      "source": [
        "test_rating_sequence = \"3,3,3,3,3,1,1,3,3,3,3,3\"\n",
        "requestRetention = 0.9  # recommended setting: 0.8 ~ 0.9\n",
        "easyBonus = 1.3\n",
        "hardInterval = 1.2\n",
        "\n",
        "t_history = \"0\"\n",
        "d_history = \"0\"\n",
        "for i in range(len(test_rating_sequence.split(','))):\n",
        "    rating = test_rating_sequence[2*i]\n",
        "    last_t = int(t_history.split(',')[-1])\n",
        "    r_history = test_rating_sequence[:2*i+1]\n",
        "    states = my_collection.states(t_history, r_history)\n",
        "    print(states)\n",
        "    next_t = max(1,round(float(np.log(requestRetention)/np.log(0.9) * states[0])))\n",
        "    if rating == '4':\n",
        "        next_t = round(next_t * easyBonus)\n",
        "    elif rating == '2':\n",
        "        next_t = round(last_t * hardInterval)\n",
        "    t_history += f',{int(next_t)}'\n",
        "    difficulty = round(float(np.log(requestRetention)/np.log(0.9) * states[1]), 1)\n",
        "    d_history += f',{difficulty}'\n",
        "print(f\"rating history: {test_rating_sequence}\")\n",
        "print(f\"interval history: {t_history}\")\n",
        "print(f\"difficulty history: {d_history}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMnk8/Ih2JAJZJ1PBkXQUBC",
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('fsrs4anki')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "8dd9a290ffd10997e0b0d411ff1325a47862ea932e0fd309ade800e0e51d2b4b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
